{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3952819,"sourceType":"datasetVersion","datasetId":1280375},{"sourceId":8747636,"sourceType":"datasetVersion","datasetId":5253304}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports and Dataset Preparation","metadata":{}},{"cell_type":"markdown","source":"**In this section, we start by importing the necessary libraries. We define the dataset directory and create separate directories for training and validation datasets. We split the dataset into training and validation sets using a ratio of 90:10. The images are then moved to their respective directories.**","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score\n\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\nimport torchvision.models as models\n\n# we define dataset directory\ndata_dir = '/kaggle/input/indian-actor-images-dataset/Bollywood Actor Images/Bollywood Actor Images'\n\n# defining paths for train and validation sets\nbase_dir = '/kaggle/working'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'val')\n\n# create directories\nos.makedirs(train_dir, exist_ok=True)\nos.makedirs(val_dir, exist_ok=True)\n\n# in this part we get list of actor directories\nactor_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n\n# split ratio 90% for train\ntrain_ratio = 0.9\n\nfor actor in actor_dirs:\n    actor_path = os.path.join(data_dir, actor)\n    images = os.listdir(actor_path)\n    \n    train_images, val_images = train_test_split(images, train_size=train_ratio, random_state=42)\n\n    # Create actor directories in train and val directories\n    train_actor_dir = os.path.join(train_dir, actor)\n    val_actor_dir = os.path.join(val_dir, actor)\n    os.makedirs(train_actor_dir, exist_ok=True)\n    os.makedirs(val_actor_dir, exist_ok=True)\n\n    # move training images\n    for img in train_images:\n        src_path = os.path.join(actor_path, img)\n        dst_path = os.path.join(train_actor_dir, img)\n        shutil.copyfile(src_path, dst_path)\n\n    # move validation images\n    for img in val_images:\n        src_path = os.path.join(actor_path, img)\n        dst_path = os.path.join(val_actor_dir, img)\n        shutil.copyfile(src_path, dst_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:33:58.626813Z","iopub.execute_input":"2024-06-24T16:33:58.627329Z","iopub.status.idle":"2024-06-24T16:34:15.903397Z","shell.execute_reply.started":"2024-06-24T16:33:58.627290Z","shell.execute_reply":"2024-06-24T16:34:15.901585Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Data Transformations and DataLoader Setup","metadata":{}},{"cell_type":"markdown","source":"**Here, we define the data transformations to be applied to the images. The images are resized, converted to tensors, and normalized. We then load the datasets using ImageFolder and create data loaders for both training and validation datasets.**","metadata":{}},{"cell_type":"code","source":"# defining transforms and load data\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # ResNet standard input size\n    transforms.RandomHorizontalFlip(),  \n    transforms.RandomRotation(10),  \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_dataset = datasets.ImageFolder(train_dir, transform=transform)\nval_dataset = datasets.ImageFolder(val_dir, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:34:28.732180Z","iopub.execute_input":"2024-06-24T16:34:28.732800Z","iopub.status.idle":"2024-06-24T16:34:28.804109Z","shell.execute_reply.started":"2024-06-24T16:34:28.732750Z","shell.execute_reply":"2024-06-24T16:34:28.802776Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Model Setup and Loading Pretrained Weights ResNet18","metadata":{}},{"cell_type":"markdown","source":"**Here, we load the ResNet-18 model and modify it to suit our specific task. The final fully connected layer of the model is adjusted to match the number of classes (135). We load pretrained weights from a specified file path and modify the state dictionary to align with our model's final layer. The model is then moved to the appropriate device (GPU if available, otherwise CPU).**","metadata":{}},{"cell_type":"code","source":"# load ResNet and modifing it\nresnet18 = models.resnet18(weights=None)  # initialize without pre-trained weights first\nnum_ftrs = resnet18.fc.in_features\nresnet18.fc = nn.Linear(num_ftrs, 135)  # 135 classes\n\n# load pretrained weights \npretrained_weights_path = '/kaggle/input/resnet18/resnet18-f37072fd.pth'\nassert os.path.isfile(pretrained_weights_path), f\"Error: Pretrained weights file '{pretrained_weights_path}' not found.\"\nstate_dict = torch.load(pretrained_weights_path, map_location=torch.device('cpu'))\n\n# adjust the state_dict for the final fully connected layer\nstate_dict['fc.weight'] = state_dict['fc.weight'][:135, :]\nstate_dict['fc.bias'] = state_dict['fc.bias'][:135]\n\nresnet18.load_state_dict(state_dict, strict=False)  \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nresnet18 = resnet18.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:34:41.220231Z","iopub.execute_input":"2024-06-24T16:34:41.220776Z","iopub.status.idle":"2024-06-24T16:34:41.608411Z","shell.execute_reply.started":"2024-06-24T16:34:41.220733Z","shell.execute_reply":"2024-06-24T16:34:41.606683Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop with Validation Accuracy Checks ResNet18","metadata":{}},{"cell_type":"markdown","source":"**This section defines the training loop and includes validation accuracy checks. We set up the loss function (Cross-Entropy Loss) and the optimizer (Adam). The model is trained for a specified number of epochs, and the training loss is calculated and printed for each epoch. After each epoch, the model is evaluated on the validation set to check the accuracy.**","metadata":{}},{"cell_type":"code","source":"# Defining loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(resnet18.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:34:48.872888Z","iopub.execute_input":"2024-06-24T16:34:48.873566Z","iopub.status.idle":"2024-06-24T16:34:48.887873Z","shell.execute_reply.started":"2024-06-24T16:34:48.873522Z","shell.execute_reply":"2024-06-24T16:34:48.886487Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# training loops\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    resnet18.train()\n    running_loss = 0.0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = resnet18(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n\n# Validation loop\nresnet18.eval()\nall_labels = []\nall_predictions = []\n\nwith torch.no_grad():\n    for inputs, labels in val_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = resnet18(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        \n        all_labels.extend(labels.cpu().numpy())\n        all_predictions.extend(predicted.cpu().numpy())\n\naccuracy = accuracy_score(all_labels, all_predictions)\nf1 = f1_score(all_labels, all_predictions, average='weighted')\n\nprint(f'Validation Accuracy: {accuracy * 100:.2f}%')\nprint(f'F1 Score: {f1:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T11:59:08.094000Z","iopub.execute_input":"2024-06-24T11:59:08.094455Z","iopub.status.idle":"2024-06-24T15:29:52.353306Z","shell.execute_reply.started":"2024-06-24T11:59:08.094416Z","shell.execute_reply":"2024-06-24T15:29:52.350308Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch [1/15], Loss: 4.7268\nEpoch [2/15], Loss: 3.6500\nEpoch [3/15], Loss: 2.7791\nEpoch [4/15], Loss: 1.8507\nEpoch [5/15], Loss: 0.9598\nEpoch [6/15], Loss: 0.3162\nEpoch [7/15], Loss: 0.0911\nEpoch [8/15], Loss: 0.0189\nEpoch [9/15], Loss: 0.0049\nEpoch [10/15], Loss: 0.0021\nEpoch [11/15], Loss: 0.0014\nEpoch [12/15], Loss: 0.0011\nEpoch [13/15], Loss: 0.0008\nEpoch [14/15], Loss: 0.0007\nEpoch [15/15], Loss: 0.0006\nValidation Accuracy: 56.15%\nF1 Score: 0.5479\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Saving the Model","metadata":{}},{"cell_type":"markdown","source":"**Finally, the model will be saved.**","metadata":{}},{"cell_type":"code","source":"torch.save(resnet18.state_dict(), 'indian_actor_resnet18.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-24T15:29:52.359226Z","iopub.execute_input":"2024-06-24T15:29:52.359651Z","iopub.status.idle":"2024-06-24T15:29:52.444042Z","shell.execute_reply.started":"2024-06-24T15:29:52.359616Z","shell.execute_reply":"2024-06-24T15:29:52.442875Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Model Setup and Loading Pretrained Weights MobileNetV2 ","metadata":{}},{"cell_type":"markdown","source":"**in this part we train our model with MobileNetV2 instead of ResNet18**","metadata":{}},{"cell_type":"code","source":"mobilenet_v2 = models.mobilenet_v2(weights=None)  # Initialize without pre-trained weights first\nnum_ftrs = mobilenet_v2.classifier[1].in_features\nmobilenet_v2.classifier[1] = nn.Linear(num_ftrs, 135)  # 135 classes\nmobilenet_v2 = mobilenet_v2.to(device)\n\n# loading pretrained weights \npretrained_weights_path = '/kaggle/input/mobilenetv2/mobilenet_v2-b0353104.pth'\nif os.path.isfile(pretrained_weights_path):\n    state_dict = torch.load(pretrained_weights_path, map_location=torch.device('cpu'))\n    state_dict['classifier.1.weight'] = state_dict['classifier.1.weight'][:135, :]\n    state_dict['classifier.1.bias'] = state_dict['classifier.1.bias'][:135]\n    mobilenet_v2.load_state_dict(state_dict, strict=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:36:16.874333Z","iopub.execute_input":"2024-06-24T18:36:16.876253Z","iopub.status.idle":"2024-06-24T18:36:16.985577Z","shell.execute_reply.started":"2024-06-24T18:36:16.876181Z","shell.execute_reply":"2024-06-24T18:36:16.983896Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop with Validation Accuracy Checks MobileNetV2\n","metadata":{}},{"cell_type":"markdown","source":"**Like before we define traning loops here**","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(mobilenet_v2.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:36:20.390018Z","iopub.execute_input":"2024-06-24T18:36:20.390430Z","iopub.status.idle":"2024-06-24T18:36:20.399276Z","shell.execute_reply.started":"2024-06-24T18:36:20.390400Z","shell.execute_reply":"2024-06-24T18:36:20.397415Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    mobilenet_v2.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = mobilenet_v2(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**validations are here**","metadata":{}},{"cell_type":"code","source":"# validation loop\nmobilenet_v2.eval()\nall_labels = []\nall_predictions = []\n\nwith torch.no_grad():\n    for inputs, labels in val_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = mobilenet_v2(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        \n        all_labels.extend(labels.cpu().numpy())\n        all_predictions.extend(predicted.cpu().numpy())\n\naccuracy = accuracy_score(all_labels, all_predictions)\nf1 = f1_score(all_labels, all_predictions, average='weighted')\n\nprint(f'MobileNetV2 Validation Accuracy: {accuracy * 100:.2f}%')\nprint(f'MobileNetV2 F1 Score: {f1:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:28:47.631350Z","iopub.status.idle":"2024-06-24T16:28:47.631926Z","shell.execute_reply.started":"2024-06-24T16:28:47.631650Z","shell.execute_reply":"2024-06-24T16:28:47.631672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the Model","metadata":{}},{"cell_type":"markdown","source":"**finally we save our model**","metadata":{}},{"cell_type":"code","source":"torch.save(mobilenet_v2.state_dict(), 'indian_actor_mobilenetv2.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:28:47.633946Z","iopub.status.idle":"2024-06-24T16:28:47.634515Z","shell.execute_reply.started":"2024-06-24T16:28:47.634247Z","shell.execute_reply":"2024-06-24T16:28:47.634269Z"},"trusted":true},"execution_count":null,"outputs":[]}]}